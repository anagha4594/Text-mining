{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling(Clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. \n",
    "\n",
    "For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics.\n",
    "\n",
    "\n",
    "#### LDA is an example of a topic model.\n",
    "\n",
    "\n",
    "LSA-Latent Semantic Analysis   \n",
    "matrix Factorization(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim \n",
    "\n",
    "# it has the package to do Topic modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim doesnot take input as Dataframe , but it takes each document with unique terms as a individual list\n",
    "\n",
    "doc1-[(t1,1),(t3,1)....]  \n",
    "doc2-[(t1,1),(t3,1),(t5,1)..]  \n",
    ".\n",
    ".\n",
    "doc-n-[(t1,2),(t2,4)...]   \n",
    "\n",
    "where tuple formed inside the list is(term,frequencyin the doc)  and each term is assigned a unique ID instead of the term in string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the basic libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000HDJXNA</td>\n",
       "      <td>1</td>\n",
       "      <td>What I recieved is not what is pictured here O...</td>\n",
       "      <td>12 26, 2012</td>\n",
       "      <td>A29YXBFTD7QUP3</td>\n",
       "      <td>HHA</td>\n",
       "      <td>Buyer be ware</td>\n",
       "      <td>1.356480e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B006KKS7XQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent unit and a pretty simple install usi...</td>\n",
       "      <td>09 20, 2013</td>\n",
       "      <td>A3IMTXFYD7CGDN</td>\n",
       "      <td>Peter W. George \"soyflakeman\"</td>\n",
       "      <td>high quality without high price</td>\n",
       "      <td>1.379635e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B002NP8XJ0</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm enjoying this keyboard, I'm getting anothe...</td>\n",
       "      <td>08 31, 2010</td>\n",
       "      <td>AXNOW20FQKHVW</td>\n",
       "      <td>B. Hayashi</td>\n",
       "      <td>Superb keyboard + solution for slow wake up an...</td>\n",
       "      <td>1.283213e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000EITTLE</td>\n",
       "      <td>4</td>\n",
       "      <td>Overall, this is a fantastic camera that I'm e...</td>\n",
       "      <td>02 3, 2008</td>\n",
       "      <td>A10KCAK279LO0W</td>\n",
       "      <td>mmcwatters \"macdadi80\"</td>\n",
       "      <td>One qualm: not great in low light</td>\n",
       "      <td>1.201997e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006CRXK4S</td>\n",
       "      <td>5</td>\n",
       "      <td>These work very well with mySamsung PN64D7000 ...</td>\n",
       "      <td>01 28, 2012</td>\n",
       "      <td>A19XXLMZXR764J</td>\n",
       "      <td>S. Garfinkle</td>\n",
       "      <td>Work great, fit well</td>\n",
       "      <td>1.327709e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall                                         reviewText  \\\n",
       "0  B000HDJXNA        1  What I recieved is not what is pictured here O...   \n",
       "1  B006KKS7XQ        5  Excellent unit and a pretty simple install usi...   \n",
       "2  B002NP8XJ0        5  I'm enjoying this keyboard, I'm getting anothe...   \n",
       "3  B000EITTLE        4  Overall, this is a fantastic camera that I'm e...   \n",
       "4  B006CRXK4S        5  These work very well with mySamsung PN64D7000 ...   \n",
       "\n",
       "    reviewTime      reviewerID                   reviewerName  \\\n",
       "0  12 26, 2012  A29YXBFTD7QUP3                            HHA   \n",
       "1  09 20, 2013  A3IMTXFYD7CGDN  Peter W. George \"soyflakeman\"   \n",
       "2  08 31, 2010   AXNOW20FQKHVW                     B. Hayashi   \n",
       "3   02 3, 2008  A10KCAK279LO0W         mmcwatters \"macdadi80\"   \n",
       "4  01 28, 2012  A19XXLMZXR764J                   S. Garfinkle   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                      Buyer be ware    1.356480e+09  \n",
       "1                    high quality without high price    1.379635e+09  \n",
       "2  Superb keyboard + solution for slow wake up an...    1.283213e+09  \n",
       "3                  One qualm: not great in low light    1.201997e+09  \n",
       "4                               Work great, fit well    1.327709e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the Modi Tweets csv\n",
    "amazon=pd.read_csv('C:\\\\Users\\\\HII\\\\Desktop\\\\Text Mining\\\\10 Text Mining\\\\amazon_reviews_big.csv')\n",
    "amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Bag of word analysis\n",
    "\n",
    "  We take text column as input and identify the individual words and their frequency of each unique word.\n",
    "  From those individual words we will come to an idea what area the words are refering .\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word size /font size depends on the frequency of the word i.e higher freq is in bigger size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=[16,14])\n",
    "text=' '.join(map(str,amazon['reviewText'].values))\n",
    "wc=WordCloud(background_color='white',colormap='plasma').generate(text)\n",
    "plt.imshow(wc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=amazon.reviewText.fillna(\"\").str.lower().str.replace('[^a-z ]','')\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([\"\",'use','will','one','good'])# to add custom stopword list to our original list ofn stopwords\n",
    "stemmer=nltk.stem.PorterStemmer()\n",
    "# Function to clean the each doc with stopwords and lemmitiation\n",
    "def clean_doc(doc):\n",
    "    words=doc.split(\" \")\n",
    "    words_clean=[stemmer.stem(w) for w in words if w not in stopwords]\n",
    "    words_clean=[w for w in words if w not in stopwords]\n",
    "    return words_clean\n",
    "docs_clean=docs.apply(clean_doc)\n",
    "docs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim converts the unique word dictionary \n",
    "dictionary=gensim.corpora.Dictionary(docs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms with their Token id\n",
    "list(zip(dictionary.keys(),dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID for Word/term Tablet\n",
    "dictionary.token2id['tablet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the bag of words in each document \n",
    "docs_bow=[]\n",
    "for i in docs_clean:\n",
    "    bow=dictionary.doc2bow(i)\n",
    "    docs_bow.append(bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# docs_bow has the input format required for gensim package to process it to do Topic classification/ modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mod=gensim.models.LdaMulticore(docs_bow,\n",
    "                               id2word=dictionary,\n",
    "                                  num_topics=4,random_state=100,iteration=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                             \n",
    "#%timeit gensim.models.LdaMulticore(docs_bow,id2word=dictionary,num_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                               \n",
    "#%timeit gensim.models.LdaModel(docs_bow,id2word=dictionary,num_topics=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document- Topic Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_mod.get_document_topics(docs_bow[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unitagging - tagging each doc with highest probable topic\n",
    "topics=[]\n",
    "for i in docs_bow:\n",
    "    df=pd.DataFrame(lda_mod.get_document_topics(i),columns=['topic','Probs'])\n",
    "    topic=df.sort_values(by='Probs').iloc[-1]['topic']\n",
    "    topics.append(topic)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon['topics']=topics  \n",
    "amazon.topics.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above topic list we take out the top 10 high probable words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mod.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
